{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Data referenced below:\n",
    "\n",
    "Task C., Bhagat K., Streat D., Simpson A., and Howarth G.S. (2023),\n",
    "NIST Diverse Communities Data Excerpts, National Institute of Standards and Technology,\n",
    "https://doi.org/10.18434/mds2-2895"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n",
    "    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "            for v in vec]\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to clean this data for use in the gradient descent algorithm\n",
    "- Predictors consist of things that you might be able to discern from a quick look at someone on the street\n",
    "    - AGEP (age), SEX, RAC1P (Race separated into different categories), DPHY (ambulatory disability e.g. wheelchair or limp), DEYE (visual impairment)\n",
    "- The idea is to train a model to predict somebodies invisible traits such as income, education, marital status, etc. based on their visible ones.\n",
    "    - As a proof of concept that this is possible with the NIST data, we will be trying to predict whether someone is above/below the poverty line (using logistic regression) and what income decile they are in (using linear regression)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [],
   "source": [
    "# Load data files\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "# import nation data from NIST\n",
    "nist_national_2019 = pd.read_csv('https://media.githubusercontent.com/media/usnistgov/SDNist/refs/heads/main/nist%20diverse%20communities%20data%20excerpts/national/national2019.csv')\n",
    "\n",
    "# remove NA values (represented as 'N' in this dataset)\n",
    "nist_national_2019 = nist_national_2019[nist_national_2019[\"AGEP\"] != \"N\"]\n",
    "nist_national_2019 = nist_national_2019[nist_national_2019[\"RAC1P\"] != \"N\"]\n",
    "nist_national_2019 = nist_national_2019[nist_national_2019[\"SEX\"] != \"N\"]\n",
    "nist_national_2019 = nist_national_2019[nist_national_2019[\"DPHY\"] != \"N\"]\n",
    "nist_national_2019 = nist_national_2019[nist_national_2019[\"POVPIP\"] != \"N\"]\n",
    "nist_national_2019 = nist_national_2019[nist_national_2019[\"PINCP_DECILE\"] != \"N\"]\n",
    "\n",
    "# scale the data to be within (-1 - 1)\n",
    "nist_national_2019[\"SEX\"] = (nist_national_2019[\"SEX\"] - 1) * 2 - 1\n",
    "nist_national_2019[\"AGEP\"] = nist_national_2019[\"AGEP\"]/99 # we can do this since AGEP has set bounds in the data_dictionary\n",
    "for i in range(9):\n",
    "    # each category of race is given its own column and a value of 1 is applied to the column that represents that race category\n",
    "    nist_national_2019[\"RAC1P-\" + str(i+1)] = nist_national_2019['RAC1P'].apply(lambda x: 1 if x == (i+1) else -1)\n",
    "# -1 if a person is not hispanic 0 otherwise\n",
    "nist_national_2019[\"HISP\"] = nist_national_2019['HISP'].apply(lambda x: -1 if x == 0 else 1)\n",
    "# 1 if a person has an ocular disability, -1 otherwise\n",
    "nist_national_2019[\"DEYE\"] = nist_national_2019['DEYE'].apply(lambda x: 1 if x == 1 else -1)\n",
    "# 1 if a person has a physical disability, -1 otherwise\n",
    "nist_national_2019[\"DPHY\"] = pd.to_numeric(nist_national_2019[\"DPHY\"]).apply(lambda x: 1 if x == 1 else -1)\n",
    "# 1 if a record is (at or) below the poverty line\n",
    "nist_national_2019[\"POVPIP\"] = pd.to_numeric(nist_national_2019[\"POVPIP\"]).apply(lambda x: 1 if x <= 100 else -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our first set of tests, we will be performing Logistic Regression, trying to predict whether someone is above/below the poverty line."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test set sizes: 17268 4318\n"
     ]
    }
   ],
   "source": [
    "X = nist_national_2019[[\"AGEP\", \"SEX\", \"HISP\", \"RAC1P-1\", \"RAC1P-2\", \"RAC1P-3\", \"RAC1P-4\", \"RAC1P-5\", \"RAC1P-6\", \"RAC1P-7\", \"RAC1P-8\", \"RAC1P-9\", \"DPHY\", \"DEYE\"]]\n",
    "y = nist_national_2019[[\"POVPIP\"]]\n",
    "\n",
    "y = np.ravel(np.array(y, dtype=float))\n",
    "X = np.array(X)\n",
    "\n",
    "# print(y.shape)\n",
    "# print(X.shape)\n",
    "\n",
    "# Split data into training and test sets\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "print('Train and test set sizes:', len(y_train), len(y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Sci-Kit Learn Implementation**\n",
    "This is a pre-built implementation of Logistic Regression that does not satisfy differential privacy.\n",
    "We can use this as a benchmark for our own implementations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.89601667438629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_model():\n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "model = train_model()\n",
    "# print('Model coefficients:', model.coef_[0])\n",
    "print('Model accuracy:', np.sum(model.predict(X_test) == y_test)/X_test.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the sci-kit learn model has very good (and very fast) results.\n",
    "But what about if we wanted to create a differentially private model to predict whether someone is below the poverty line?\n",
    "\n",
    "First we need a few functions to allow us to perform both linear and logistic regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [],
   "source": [
    "\n",
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "\n",
    "# This is the logistic loss function.\n",
    "def logistic_loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# Mean Square Error for use in linear regression\n",
    "def linear_loss(theta, xi, yi):\n",
    "    return np.mean((yi - xi) ** 2)\n",
    "\n",
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def logistic_gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "# Mean Square Error gradient for use in linear regression\n",
    "def linear_gradient(theta, X, y):\n",
    "    N = X.shape[0]\n",
    "    y = np.array(y, dtype=float)\n",
    "    predictions = X @ theta\n",
    "    residuals = y - predictions\n",
    "    return -2 / N * X.T @ residuals\n",
    "\n",
    "# function to get a single prediction for a model based on an input\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "def linear_predict(xi, theta, bias=0):\n",
    "    label = ((xi @ theta + bias).astype(int)).clip(min=0, max=9)\n",
    "    # print(label)\n",
    "    return label\n",
    "\n",
    "# accuracy heuristic for comparing our different algorithms\n",
    "def accuracy(theta):\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    # print(predict(X_test, theta))\n",
    "    # print(y_test)\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]\n",
    "\n",
    "def linear_accuracy(theta):\n",
    "    # print(y_test)\n",
    "    return np.sum(linear_predict(X_test, theta) == y_test.astype(int))/X_test.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following implementation does not satisfy differential privacy, but it's also another good baseline for us to compare our differentially private models to."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression Accuracy: 0.8967114404817045\n"
     ]
    }
   ],
   "source": [
    "# function to get the average gradients in a model. Only used in\n",
    "def avg_grad(theta, X, y, gradient):\n",
    "    assert len(X) == len(y)\n",
    "    gradients = [gradient(theta, X[i], y[i]) for i in range(len(X))]\n",
    "\n",
    "    avg_gradient = np.mean(gradients, axis=0)\n",
    "    return avg_gradient\n",
    "\n",
    "# gradient descent algorithm. runs for a set number of iterations, and allows the user to specify which loss function to use (logistic/linear)\n",
    "def gradient_descent(iterations, gradient):\n",
    "    theta = [0 for _ in range(X_test.shape[1])]\n",
    "    for iteration in range(iterations):\n",
    "        theta = theta - avg_grad(theta, X_train, y_train, gradient)\n",
    "    return theta\n",
    "\n",
    "theta = gradient_descent(200, logistic_gradient)\n",
    "print(\"Baseline Logistic Regression Accuracy: \" + str(accuracy(theta)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now modify the gradient descent algorithm that we used previously to add noise to the gradients in each training iteration.\n",
    "There's a few questions:\n",
    "- How much privacy cost should we allocate when adding noise?\n",
    "- How much noise do we have to add to satisfy epsilon-delta differential privacy?\n",
    "\n",
    "The main issue is with the sensitivity of the gaussian_mech_vec. We can call the L2_clip() function on each gradient to clip it to a constant value. This allows us to control the sensitivity of the gaussian_mech_vec() function, as it is the same as whatever value we choose to clip the gradients to.\n",
    "In the code below that value can be controlled by the L2_clip_param argument in the noisy_gradient_descent() function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "\n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "def noisy_gradient_descent(iterations, epsilon, delta, L2_clip_param, gradient):\n",
    "    theta = np.array([0 for _ in range(X_test.shape[1])])\n",
    "    for iteration in range(iterations):\n",
    "        gradients = [gradient(theta, X_train[i], y_train[i]) for i in range(len(X_train))]\n",
    "        clipped_grads = [L2_clip(g, L2_clip_param) for g in gradients]\n",
    "        grad_sum = np.sum(clipped_grads, axis=0)\n",
    "\n",
    "        noisy_grad_sum = gaussian_mech_vec(grad_sum, sensitivity=L2_clip_param, epsilon= (epsilon / iterations), delta = (delta/iterations))\n",
    "        # reveals size of training data\n",
    "        noisy_grad = np.array(noisy_grad_sum) / len(X_train)\n",
    "        theta = theta - noisy_grad\n",
    "    return theta\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In standard stochastic gradient descent, we use every piece of training data in every iteration of training.\n",
    "The main difference here in the mini-batch algorithm is that we only train on a subset of the training data (randomly chosen). This allows us to train more quickly due to the smaller iteration sizes, but still hopefully get similar results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "def noisy_gradient_descent_batching(iterations, epsilon, delta, L2_clip_param, batch_size, gradient):\n",
    "    theta = np.array([0 for _ in range(X_test.shape[1])])\n",
    "    for iteration in range(iterations):\n",
    "        batch_indices = []\n",
    "        # build the batch indices using random sampling\n",
    "        while(len(batch_indices) != batch_size):\n",
    "            rand = np.random.randint(0, len(X_train))\n",
    "            if rand not in batch_indices:\n",
    "                batch_indices.append(rand)\n",
    "        gradients = [gradient(theta, X_train[i], y_train[i]) for i in batch_indices]\n",
    "        clipped_grads = [L2_clip(g, L2_clip_param) for g in gradients] #L2 sensitivity is 5\n",
    "        grad_sum = np.sum(clipped_grads, axis=0)\n",
    "\n",
    "        noisy_grad_sum = gaussian_mech_vec(grad_sum, sensitivity=L2_clip_param, epsilon= (epsilon / iterations), delta = (delta/iterations))\n",
    "        # reveals size of training data\n",
    "        noisy_grad = np.array(noisy_grad_sum) / len(X_train)\n",
    "        theta = theta - noisy_grad\n",
    "    return theta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy Gradient Descent Accuracy: 0.8471514590088004\n",
      "Noisy Gradient Descent Time: 3.001326084136963\n",
      "Noisy Mini-batch Descent Accuracy: 0.8967114404817045\n",
      "Noisy Mini-batch Descent Time: 0.8987910747528076\n"
     ]
    }
   ],
   "source": [
    "s_gd = time.time()\n",
    "theta = noisy_gradient_descent(50, 1.0, 1e-5, 5, logistic_gradient)\n",
    "e_gd = time.time()\n",
    "\n",
    "s_bgd = time.time()\n",
    "theta_prime = noisy_gradient_descent_batching(50, 1.0, 1e-5, 5, int(X_train.shape[0]/10), logistic_gradient)\n",
    "e_bgd = time.time()\n",
    "print('Noisy Gradient Descent Accuracy:', accuracy(theta))\n",
    "print('Noisy Gradient Descent Time:', e_gd-s_gd)\n",
    "print('Noisy Mini-batch Descent Accuracy:', accuracy(theta_prime))\n",
    "print('Noisy Mini-batch Descent Time:', e_bgd-s_bgd)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We were able to successfully predict whether someone was above the poverty line based on several (visible) columns.\n",
    "It would also be interesting to be able to predict something in a non-binary manner. Such as the decile of income of a person.\n",
    "\n",
    "Below we set the y (prediction) set to be the Principle Decile column from the NIST dataset, and then split up the data into training/test data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test set sizes: 17268 4318\n"
     ]
    }
   ],
   "source": [
    "# Load data files\n",
    "\n",
    "X = nist_national_2019[[\"AGEP\", \"SEX\", \"HISP\", \"RAC1P-1\", \"RAC1P-2\", \"RAC1P-3\", \"RAC1P-4\", \"RAC1P-5\", \"RAC1P-6\", \"RAC1P-7\", \"RAC1P-8\", \"RAC1P-9\", \"DPHY\", \"DEYE\"]]\n",
    "y = nist_national_2019[[\"PINCP_DECILE\"]]\n",
    "\n",
    "y = np.ravel(np.array(y))\n",
    "X = np.array(X)\n",
    "\n",
    "# Split data into training and test sets\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "print('Train and test set sizes:', len(y_train), len(y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "def noisy_gradient_descent_linear(iterations, epsilon, delta, L2_clip_param, gradient):\n",
    "    theta = np.array([0 for _ in range(X_test.shape[1])])\n",
    "    for iteration in range(iterations):\n",
    "        gradients = gradient(theta, X_train, y_train)\n",
    "        clipped_grad = L2_clip(gradients, L2_clip_param)\n",
    "        noisy_grad = gaussian_mech(clipped_grad, sensitivity=L2_clip_param, epsilon= (epsilon / iterations), delta = (delta/iterations))\n",
    "        theta = theta - noisy_grad\n",
    "    return theta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [],
   "source": [
    "def noisy_gradient_descent_batching_linear(iterations, epsilon, delta, L2_clip_param, batch_size, gradient):\n",
    "    theta = np.array([0 for _ in range(X_test.shape[1])])\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        batch_indices = []\n",
    "        # build the batch indices using random sampling\n",
    "        while(len(batch_indices) != batch_size):\n",
    "            rand = np.random.randint(0, len(X_train))\n",
    "            if rand not in batch_indices:\n",
    "                batch_indices.append(rand)\n",
    "        X_set = np.array([X_train[i] for i in batch_indices])\n",
    "        y_set = np.array([y_train[i] for i in batch_indices])\n",
    "        gradients = gradient(theta, X_set, y_set)\n",
    "        clipped_grad = L2_clip(gradients, L2_clip_param)\n",
    "        noisy_grad = gaussian_mech(clipped_grad, sensitivity=L2_clip_param, epsilon= (epsilon / iterations), delta = (delta/iterations))\n",
    "        theta = theta - noisy_grad\n",
    "    return theta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy Gradient Descent Accuracy: 0.182260305697082\n",
      "Noisy Gradient Descent Time: 0.7623159885406494\n",
      "Noisy Mini-batch Descent Accuracy: 0.1000463177396943\n",
      "Noisy Mini-batch Descent Time: 0.20416569709777832\n"
     ]
    }
   ],
   "source": [
    "s_gd = time.time()\n",
    "theta = noisy_gradient_descent_linear(500, 1.0, 1e-5, 5, linear_gradient)\n",
    "e_gd = time.time()\n",
    "\n",
    "s_bgd = time.time()\n",
    "theta_prime = noisy_gradient_descent_batching_linear(500, 1.0, 1e-5, 5, int(X_train.shape[0]/100), linear_gradient)\n",
    "e_bgd = time.time()\n",
    "print('Noisy Gradient Descent Accuracy:', linear_accuracy(theta))\n",
    "print('Noisy Gradient Descent Time:', e_gd-s_gd)\n",
    "print('Noisy Mini-batch Descent Accuracy:', linear_accuracy(theta_prime))\n",
    "print('Noisy Mini-batch Descent Time:', e_bgd-s_bgd)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No matter how we change our parameters, it seems like linear regression is not able to find a solution, and generally ends with <25% accuracy. This is probably due to several factors:\n",
    "1. We are trying to predict a column with 10 different possibilities using only 14 predictors. This is a fairly complex problem\n",
    "2. A persons income decile is a complex issue that is not easily able to be determined. We had much more success with the poverty line question (using logistic regression) because there are only two possibilities; a person is either above or below the poverty line.\n",
    "\n",
    "The added complexity of predicting a persons income decile is too complex to predict just based on their visual predictors. Perhaps adding more predictors would make this more feasible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
